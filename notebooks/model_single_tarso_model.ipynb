{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616f8712",
   "metadata": {},
   "source": [
    "# Single Tarso (Pastas) model for one time series\n",
    "\n",
    "This notebook creates a minimal Pastas model (Tarso-style) using a random timeseries\n",
    "`output_data/only_csv_wiertsema/time_series.csv` as the observed head.\n",
    "\n",
    "Steps:\n",
    "1. Find the repository root using relative paths.\n",
    "2. Load the CSV and pick the first numeric column as the observed series.\n",
    "3. Try to load a precipitation stress from `input_stressors/prec_station_249.csv` (if present), otherwise create a small synthetic stress.\n",
    "4. Build a tiny Pastas model with a Gamma response and fit it.\n",
    "\n",
    "Run the cells below in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bd8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: d:\\Users\\jvanruitenbeek\\data_validation\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: setup imports and repo discovery\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pastas as ps\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# find repo root\n",
    "repo_root = Path.cwd()\n",
    "for candidate in [repo_root] + list(repo_root.parents):\n",
    "    if (candidate / 'pyproject.toml').exists() or (candidate / '.git').exists():\n",
    "        repo_root = candidate\n",
    "        break\n",
    "print('Repo root:', repo_root)\n",
    "\n",
    "wiertsema_dir = repo_root / 'output_data' / 'only_csv_wiertsema'\n",
    "fugro_dir = repo_root / 'output_data' / 'only_csv_fugro'    \n",
    "singles_dir = repo_root / 'output_data' / 'only_csv_singles'\n",
    "stressor_dir = repo_root / 'input_stressors'\n",
    "out_fig = repo_root / 'output_data' / 'figures'\n",
    "out_fig.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7937859",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['DateTime', 'head']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      5\u001b[39m name_model = \u001b[33m\"\u001b[39m\u001b[33m86349-1 HB011PB01 HB_BE0147+68_BUKR_GMW_PB1_F-247\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# # --- Load and clean the head data ---------------------------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# df_head = pd.read_csv(\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     Path(wiertsema_dir) / (name_model + \".csv\"),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#--- Load and clean the head data ---------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df_head = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwiertsema_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDateTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhead\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDateTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDateTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8-sig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Take first numeric column and rename to 'head'\u001b[39;00m\n\u001b[32m     26\u001b[39m head = df_head.select_dtypes(\u001b[33m\"\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m\"\u001b[39m).iloc[:, \u001b[32m0\u001b[39m].rename(\u001b[33m\"\u001b[39m\u001b[33mhead\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.orig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.usecols_dtype == \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols).issubset(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_names\n\u001b[32m    139\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.names) > \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\jvanruitenbeek\\data_validation\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:988\u001b[39m, in \u001b[36mParserBase._validate_usecols_names\u001b[39m\u001b[34m(self, usecols, names)\u001b[39m\n\u001b[32m    986\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    989\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m     )\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[31mValueError\u001b[39m: Usecols do not match columns, columns expected but not found: ['DateTime', 'head']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Name of the directory containing single well data\n",
    "name_model = \"86349-1 HB011PB01 HB_BE0147+68_BUKR_GMW_PB1_F-247\"\n",
    "\n",
    "# # --- Load and clean the head data ---------------------------------------------\n",
    "# df_head = pd.read_csv(\n",
    "#     Path(wiertsema_dir) / (name_model + \".csv\"),\n",
    "#     index_col=0,\n",
    "#     parse_dates=True,\n",
    "#     encoding=\"utf-8-sig\"\n",
    "# )\n",
    "\n",
    "#--- Load and clean the head data ---------------------------------------------\n",
    "df_head = pd.read_csv(\n",
    "    Path(wiertsema_dir) / (name_model + \".csv\"),\n",
    "    usecols=[\"DateTime\",\"head\"],\n",
    "    parse_dates=[\"DateTime\"],\n",
    "    dayfirst=True,\n",
    "    index_col=\"DateTime\",\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "# Take first numeric column and rename to 'head'\n",
    "head = df_head.select_dtypes(\"number\").iloc[:, 0].rename(\"head\")\n",
    "\n",
    "# Crop to first and last valid (non-NaN) data points\n",
    "df_head = head.loc[head.first_valid_index(): head.last_valid_index()]\n",
    "\n",
    "\n",
    "df_prec = pd.read_csv(Path(stressor_dir) / \"prec_station_249_2020.csv\",\n",
    "                   index_col=0, parse_dates=True)[\"Precipitation\"]\n",
    "\n",
    "df_evap = pd.read_csv(Path(stressor_dir) / \"evap_station_249_2020.csv\",\n",
    "                   index_col=0, parse_dates=True)[\"ET\"]\n",
    "\n",
    "df_prec_long = pd.read_csv(Path(stressor_dir) / \"precipitation_long_clean.csv\",\n",
    "                   index_col=0, parse_dates=True)[\"precipitation\"]\n",
    "\n",
    "df_evap_long = pd.read_csv(Path(stressor_dir) / \"evaporation_long_clean.csv\",\n",
    "                   index_col=0, parse_dates=True)[\"evaporation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8876b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_slice_date = \"2024-02-15\"\n",
    "# end_slice_date = \"2024-12-03\"\n",
    "\n",
    "# # Slicing the data \n",
    "# df_slice = df_head.loc[start_slice_date:end_slice_date]\n",
    "\n",
    "# Modifying df_head to only include data between start_slice_date and end_slice_date\n",
    "#df_slice.info()\n",
    "df_head.sort_index().resample(\"D\").last()\n",
    "\n",
    "#Converting values to NaN\n",
    "df_head_clean = df_head.where(df_head >= -450)\n",
    "\n",
    "# Converting to m and plotting\n",
    "df_head_m = df_head_clean / 100\n",
    "df_head_m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb641ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec.plot()\n",
    "df_prec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evap.plot()\n",
    "df_evap.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pastas as ps\n",
    "\n",
    "# --- Prepare series from your current variables --------------------------------\n",
    "# If df_head is a DataFrame, grab the first numeric column; else keep as Series.\n",
    "head = df_head_m\n",
    "prec = df_prec.rename(\"precip\")\n",
    "evap = df_evap.rename(\"evap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and configure the first model\n",
    "ml = ps.Model(head, name=\"Model\")\n",
    "ts1 = ps.RechargeModel(\n",
    "    prec=prec,\n",
    "    evap=evap,\n",
    "    rfunc=ps.Exponential(),\n",
    "    recharge=ps.rch.Linear(),\n",
    "    name=\"recharge\",\n",
    ")\n",
    "ml.add_stressmodel(ts1)\n",
    "ml.add_transform(ps.ThresholdTransform())\n",
    "# mlt.del_noisemodel()\n",
    "ml.solve(report=True, warmup=3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64076455",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = ml.stressmodels[\"recharge\"]\n",
    "prec, evap = sm.stress[0].series_original, sm.stress[1].series_original\n",
    "\n",
    "# delete all the stressmodels, the constant and the transform\n",
    "ml.del_stressmodel(\"recharge\")\n",
    "ml.del_constant()\n",
    "ml.del_transform()\n",
    "\n",
    "# then add a TarsoModel\n",
    "sm = ps.TarsoModel(prec, evap, dmin=head.min(), dmax=head.max(), name=\"Tarso\")\n",
    "ml.add_stressmodel(sm)\n",
    "\n",
    "# --- Constrain time scales & evap factor --------------------------------------\n",
    "ml.set_parameter(\"Tarso_a0\", pmin=0,  pmax=200.0)   # days\n",
    "ml.set_parameter(\"Tarso_a1\", pmin=10.0, pmax=200.0)  # days\n",
    "\n",
    "\n",
    "\n",
    "# and solve and plot the results again\n",
    "ml.solve(report=True)\n",
    "ml.plots.results(figsize=(10,4))\n",
    "\n",
    "# Saving model to file\n",
    "ml.to_file(name_model + \"_tarso_model.pas\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.get_response_tmax(\"Tarso\", cutoff=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie om karakterestieken van de block response te krijgen\n",
    "def get_peakblock_t95(param):\n",
    "    \"\"\"\n",
    "    Computes peak block and characteristic response time (t95) for\n",
    "    an exponential model with given parameters.\n",
    "\n",
    "    Parameters:\n",
    "        param (dict): Dictionary containing model parameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Piek_e, t95_e, Piek_0, t95_0) - peak block and t95 for both upper (_e) and lower regime (_0).\n",
    "    \"\"\"\n",
    "    A1, a1, A0, a0, d0, d1 = (\n",
    "        param[\"Tarso_A1\"],\n",
    "        param[\"Tarso_a1\"],\n",
    "        param[\"Tarso_A0\"],\n",
    "        param[\"Tarso_a0\"],\n",
    "        param[\"Tarso_d0\"],\n",
    "        param[\"Tarso_d1\"],\n",
    "    )\n",
    "\n",
    "    # Compute equivalent parameters\n",
    "    A_e = 1 / ((1 / A0) + (1 / A1))\n",
    "    (A1 / (A0 + A1)) * d0 + (A0 / (A0 + A1)) * d1\n",
    "    a_e = (a1 / A1) * A_e\n",
    "\n",
    "    # Compute characteristic response times\n",
    "    t95_e = ps.Exponential().get_tmax(p=(A_e, a_e), cutoff=0.95)\n",
    "    t95_0 = ps.Exponential().get_tmax(p=(A0, a0), cutoff=0.95)\n",
    "\n",
    "    # Compute peak blocks\n",
    "    Piek_e = ps.Exponential(meanstress=1, cutoff=0.95).block((A_e, a_e))[0]\n",
    "    Piek_0 = ps.Exponential(meanstress=1, cutoff=0.95).block((A0, a0))[0]\n",
    "\n",
    "    return Piek_e, t95_e, Piek_0, t95_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Map ml.get_parameters() -> your dict format\n",
    "vals = ml.get_parameters()  # array([A0, a0, d0, A1, a1, d1, f])\n",
    "\n",
    "param = {\n",
    "    \"Tarso_A0\": float(vals[0]),\n",
    "    \"Tarso_a0\": float(vals[1]),\n",
    "    \"Tarso_d0\": float(vals[2]),\n",
    "    \"Tarso_A1\": float(vals[3]),\n",
    "    \"Tarso_a1\": float(vals[4]),\n",
    "    \"Tarso_d1\": float(vals[5]),\n",
    "    # not used by your function, but available:\n",
    "    \"Tarso_f\":  float(vals[6]),\n",
    "}\n",
    "\n",
    "# --- Now call your function\n",
    "Piek_e, t95_e, Piek_0, t95_0 = get_peakblock_t95(param)\n",
    "\n",
    "print(f\"Peak block (equivalent): {Piek_e:.6g}\")\n",
    "print(f\"t95 (equivalent):        {t95_e:.2f} days\")\n",
    "print(f\"Peak block (A0,a0):      {Piek_0:.6g}\")\n",
    "print(f\"t95 (A0,a0):             {t95_0:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fa462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall time span (days)\n",
    "span_days = (df_head.index.max() - df_head.index.min()) / pd.Timedelta(days=1)\n",
    "print(\"Total span (days):\", span_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b12297",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-validation-py3.12 (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
